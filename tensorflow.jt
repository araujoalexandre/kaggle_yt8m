#!/bin/bash

#BSUB -J test
#BSUB -gpu "num=4:mode=exclusive_process:mps=no:j_exclusive=yes"
#BSUB -n 8,11
#BSUB -R "span[ptile=1]"
# Specify location for standard output, standard error.  Adjust as needed.
#BSUB -o /linkhome/rech/grpgen/ubx31mc/%J.tensorflow.out
#BSUB -e /linkhome/rech/grpgen/ubx31mc/%J.tensorflow.err

ml anaconda/py3 cudnn nccl
source activate tensorflow1.8-py3

# config file start with the data type on the first line [video | frame]
# the exec args can be parse from the second line :
# --args1 value1 \
# --args2 value 2 ... 
CONFIG_FILE="/linkhome/rech/grpgen/ubx31mc/kaggle_yt8m/config.cfg"
DATA_TYPE=$(head $CONFIG_FILE --lines 1)
PYTHON_ARGS=$(tail $CONFIG_FILE -n +2 | sed -e ':a' -e 'N' -e '$!ba' -e 's/\\\n/ /g')


DATE_STR=$(date '+%Y-%m-%d_%H.%M')

EXEC_DIR="/linkhome/rech/grpgen/ubx31mc/kaggle_yt8m/google_youtube-8m"
DATA_DIR="/pwrwork/rech/jvd/ubx31mc/yt8m/${DATA_TYPE}"

TRAIN_DIR="/pwrwork/rech/jvd/ubx31mc/yt8m_models/${DATA_TYPE}/${DATE_STR}"
LOGS_DIR="/pwrwork/rech/jvd/ubx31mc/yt8m_models/${DATA_TYPE}/${DATE_STR}_logs"
mkdir ${TRAIN_DIR}
mkdir ${LOGS_DIR}

WORKER_PORT=2222
PS_PORT=2223


HOST=false
CORE=false
for STR in $LSB_MCPU_HOSTS
do
  if [ $HOST = false ]; then
    HOST_STR=$STR
    HOST=true
  else
    CORE_STR=$STR
    CORE=true
  fi

  if [ $HOST = true ] && [ $CORE = true ]; then
    if [[ -z $TMP_HOSTS ]]
    then
      TMP_HOSTS=$HOST_STR
    else
      TMP_HOSTS="$TMP_HOSTS $HOST_STR"
    fi

    HOST=false
    CORE=false
  fi
done

index=0
tf_worker=""
for host in $TMP_HOSTS
do
  # index equal 0
  if [ $index -eq 0 ]
  then
    tf_master=[\"${host}:${WORKER_PORT}\"]
  fi
  # index equal 0
  if [ $index -eq 0 ]
  then
    tf_ps=[\"${host}:${PS_PORT}\"]
  fi
  # index great than 0
  if [ $index -gt 0 ]
  then
    tf_worker=${tf_worker}\"${host}:${WORKER_PORT}\",
  fi
let index=$index+1
done
tf_worker=[${tf_worker::-1}]


function format_tf_config {
  echo "{\"cluster\":{\"master\":${tf_master},\"ps\":${tf_ps},\"worker\":${tf_worker}},\"task\":{\"type\":\"$1\", \"index\":$2}, \"environment\":\"cloud\"}"
}

function format_redirect {
  echo "${LOGS_DIR}/log_$1_$2.logs"
}


CWD="cd $EXEC_DIR; "
LIB="export LD_LIBRARY_PATH=/pwrlocal/pub/openmpi/2.1.2/arch/gcc-4.8/lib:/pwrlocal/pub/cudnn/7.1.4/lib64:/pwrlocal/pub/nccl/2.2.13-1/lib:$LD_LIBRARY_PATH; "
EXEC="python3 train.py ${PYTHON_ARGS}"
echo ${EXEC}

index=0
FIRST_WORKER=""
for host in $TMP_HOSTS
do
  if [ $index -eq 0 ]
  then
    # launch parameters server
    TF_CONFIG="export TF_CONFIG='$(format_tf_config ps $index)'; "
    CUDA="export CUDA_VISIBLE_DEVICES='';"
    REDIRECT=$(format_redirect ps $index)
    ARG_GPU=" --num_gpu 0"
    blaunch -no-wait -z $host "${LIB}${CWD}${TF_CONFIG}${CUDA}${EXEC}${ARG_GPU} &> ${REDIRECT}" &
    sleep 5

    # launch master
    REDIRECT=$(format_redirect master $index)
    TF_CONFIG="export TF_CONFIG='$(format_tf_config master $index)'; "
    # keep like this for the rest of the for loop
    CUDA="export CUDA_VISIBLE_DEVICES='0,1,2,3'; "
    ARG_GPU=" --num_gpu 0"
    blaunch -no-wait -z $host "${LIB}${CWD}${TF_CONFIG}${CUDA}${EXEC}${ARG_GPU} &> ${REDIRECT}" &
  else
    # launch worker
    let worker_index=$index-1
    REDIRECT=$(format_redirect worker $worker_index)
    TF_CONFIG="export TF_CONFIG='$(format_tf_config worker $worker_index)'; "
    blaunch -no-wait -z $host "${LIB}${CWD}${TF_CONFIG}${CUDA}${EXEC}${ARG_GPU} &> ${REDIRECT}" &
  fi

  let index=$index+1
done

